## Overview

Audio Text contains the text file with their output.
Text to speech is the Google Collab link where I have practically implemented all the researched models
The video link is provided in the Video_link file in the drive link.
Bark is a transformer-based text-to-speech (TTS) model developed by Suno AI under the suno-ai/bark project. It aims to convert text input into natural-sounding speech output. Bark comprises several primary models designed to handle different aspects of the TTS process, ensuring accurate representation and synthesis of human speech.

## Components
Bark consists of four primary models, each serving a specific role in the text-to-speech pipeline:

### 1. BarkSemanticModel
Also referred to as the "text" model, BarkSemanticModel is a causal auto-regressive transformer model. It focuses on predicting semantic text tokens that effectively capture the meaning of the tokenized input text. This model plays a crucial role in understanding the textual content and extracting meaningful representations for further processing.

### 2. BarkCoarseModel
BarkCoarseModel, also known as the "coarse acoustics" model, is a causal autoregressive transformer. It utilizes the output generated by the BarkSemanticModel as its input. The primary objective of this model is to forecast the initial two audio codebooks required by EnCodec, an integral part of the TTS system. By capturing coarse acoustic features, this model contributes to generating the fundamental elements needed for synthesizing speech.

### 3. BarkFineModel
The BarkFineModel, often referred to as the "fine acoustics" model, operates as a non-causal autoencoder transformer. It iteratively predicts the final codebooks by aggregating the embeddings from all preceding codebooks. This model focuses on refining the acoustic representations generated by earlier stages, enhancing the quality and fidelity of the synthesized speech output.

## Other Explored Text-to-Speech Models:
- **Tacotron**: Tacotron is a sequence-to-sequence model for generating speech directly from text. It employs an attention mechanism to align input characters with output speech features.
- **Tacotron2**: An improved version of Tacotron, Tacotron2 utilizes a modified architecture with a combination of convolutional and recurrent neural networks, resulting in better speech quality and stability.
- **Transformer TTS**: Transformer TTS is a TTS model based on the transformer architecture, which is known for its parallelization and scalability. It has been widely adopted for its ability to generate high-quality speech with faster training times.
- **Flowtron**: Flowtron is a flow-based generative model designed for high-quality text-to-speech synthesis. It leverages autoregressive flows to model the conditional distribution of spectrograms given input text.
- **FastSpeech2**: FastSpeech2 is a non-autoregressive TTS model capable of generating speech in parallel. It employs duration prediction networks to generate phoneme durations and feed-forward transformer networks for spectrogram prediction.
- **FastPitch**: FastPitch is another non-autoregressive TTS model that focuses on predicting fundamental frequency (F0) contours directly from input text, enabling faster speech synthesis.
- **TalkNet**: TalkNet is a neural network-based TTS model that combines deep learning techniques with traditional signal processing methods for speech synthesis.
- **Facebook MMS TTS (facebook/mms-tts-eng)**: A text-to-speech model developed by Facebook, MMS TTS is designed to produce high-quality speech output from input text.

## Vocoders (Mel-spec to Audio):
In addition to the text-to-speech models mentioned above, several vocoders are commonly used to convert mel-spectrograms into audio waveforms. Some popular examples include:

- **WaveNet**: WaveNet is a deep generative model developed by DeepMind for synthesizing raw audio waveforms. It employs dilated convolutions to capture long-range dependencies in the audio signal.
- **WaveGlow**: WaveGlow is a flow-based neural network architecture designed for high-quality and efficient audio synthesis. It utilizes invertible 1x1 convolutions and affine coupling layers to model the conditional distribution of audio waveforms.
- **HiFiGAN**: HiFiGAN is a generative adversarial network (GAN) architecture specifically designed for high-fidelity audio generation. It focuses on generating realistic audio waveforms with improved perceptual quality.
